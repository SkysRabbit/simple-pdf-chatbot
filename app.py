# frontend app 
import streamlit as st
import sys
from langchain_community.vectorstores.chroma import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from query_data import query_rag

st.set_page_config(
   page_title='å°è©±æ©Ÿå™¨äººforä¸Šæ°´',
   page_icon="ğŸ¦ˆ",
   layout='centered',
   initial_sidebar_state = 'expanded',
   menu_items = {
      'About': ''':rainbow[å°è©±æ©Ÿå™¨äººè§£æ±ºå·¥ä½œäººå“¡çš„ç–‘å•]'''
   }
)

st.title('ğŸ¤–ä¸Šæ°´å°è©±æ©Ÿå™¨äºº')
st.caption('å°è©±æ©Ÿå™¨äººä½¿ç”¨LlamağŸ¦™')

if 'messages' not in st.session_state:
   st.session_state['messages'] = [{'role': 'assistant', 'content': 'ä½ å¥½ï¼Œæœ‰ä»€éº¼å¯ä»¥ç‚ºä½ å›ç­”çš„ï¼Ÿ'}]

for msg in st.session_state['messages']:
   if msg['role'] == 'assistant':
      st.chat_message('assistant', avatar='ğŸ¦™').write(msg['content'])
   else:
      st.chat_message('user', avatar='ğŸ§¸').write(msg['content'])

if prompt := st.chat_input():
   try:
      st.session_state['messages'].append({'role': 'user', 'content': prompt})
      st.chat_message('user', avatar='ğŸ§¸').write(prompt)
      response = query_rag(prompt)
      # #for streaming in frontend
      # with st.chat_message('assistant', avatar='ğŸ¦™'):
      #    response = st.write_stream(query_rag(prompt))
      st.chat_message('assistant', avatar='ğŸ¦™').markdown(response)
      st.session_state['messages'].append({'role': 'assistant', 'content': response})
   except:
      st.error(sys.exc_info[:])